["tf:trainer"]
name = "TensorFlow trainer"
type = "Template"
description = "A trainer module based on tf.contrib.learn API."
body = ["\"\"\"This module handles training and evaluation of a neural network model.", "", "Invoke the following command to train the model:", "python -m trainer --model=cnn --dataset=mnist", "", "You can then monitor the logs on Tensorboard:", "tensorboard --logdir=output\"\"\"", "", "from __future__ import absolute_import", "from __future__ import division", "from __future__ import print_function", "", "import tensorflow as tf", "", "tf.logging.set_verbosity(tf.logging.INFO)", "", "tf.flags.DEFINE_string(\"model\", \"\", \"Model name.\")", "tf.flags.DEFINE_string(\"dataset\", \"\", \"Dataset name.\")", "tf.flags.DEFINE_string(\"output_dir\", \"\", \"Optional output dir.\")", "tf.flags.DEFINE_string(\"schedule\", \"train_and_evaluate\", \"Schedule.\")", "tf.flags.DEFINE_string(\"hparams\", \"\", \"Hyper parameters.\")", "tf.flags.DEFINE_integer(\"num_epochs\", 100000, \"Number of training epochs.\")", "tf.flags.DEFINE_integer(\"save_summary_steps\", 10, \"Summary steps.\")", "tf.flags.DEFINE_integer(\"save_checkpoints_steps\", 10, \"Checkpoint steps.\")", "tf.flags.DEFINE_integer(\"eval_steps\", None, \"Number of eval steps.\")", "tf.flags.DEFINE_integer(\"eval_frequency\", 10, \"Eval frequency.\")", "", "FLAGS = tf.flags.FLAGS", "", "MODELS = {", "\t# This is a dictionary of models, the keys are model names, and the values", "\t# are the module containing get_params, model, and eval_metrics.", "\t# Example: \"cnn\": cnn", "}", "", "DATASETS = {", "\t# This is a dictionary of datasets, the keys are dataset names, and the", "\t# values are the module containing get_params, prepare, read, and parse.", "\t# Example: \"mnist\": mnist", "}", "", "HPARAMS = {", "\t\"optimizer\": \"Adam\",", "\t\"learning_rate\": 0.001,", "\t\"decay_steps\": 10000,", "\t\"batch_size\": 128", "}", "", "def get_params():", "\t\"\"\"Aggregates and returns hyper parameters.\"\"\"", "\thparams = HPARAMS", "\thparams.update(DATASETS[FLAGS.dataset].get_params())", "\thparams.update(MODELS[FLAGS.model].get_params())", "", "\thparams = tf.contrib.training.HParams(**hparams)", "\thparams.parse(FLAGS.hparams)", "", "\treturn hparams", "", "def make_input_fn(mode, params):", "\t\"\"\"Returns an input function to read the dataset.\"\"\"", "\tdef _input_fn():", "\t\tdataset = DATASETS[FLAGS.dataset].read(mode)", "\t\tif mode == tf.estimator.ModeKeys.TRAIN:", "\t\t\tdataset = dataset.repeat(FLAGS.num_epochs)", "\t\t\tdataset = dataset.shuffle(params.batch_size * 5)", "\t\tdataset = dataset.map(", "\t\t\tDATASETS[FLAGS.dataset].parse, num_threads=8)", "\t\tdataset = dataset.batch(params.batch_size)", "\t\titerator = dataset.make_one_shot_iterator()", "\t\tfeatures, labels = iterator.get_next()", "\t\treturn features, labels", "\treturn _input_fn", "", "def make_model_fn():", "\t\"\"\"Returns a model function.\"\"\"", "\tdef _model_fn(features, labels, mode, params):", "\t\tmodel_fn = MODELS[FLAGS.model].model", "\t\tglobal_step = tf.train.get_or_create_global_step()", "\t\tpredictions, loss = model_fn(features, labels, mode, params)", "", "\t\ttrain_op = None", "\t\tif mode == tf.estimator.ModeKeys.TRAIN:", "\t\t\tdef _decay(learning_rate, global_step):", "\t\t\t\tlearning_rate = tf.train.exponential_decay(", "\t\t\t\t\tlearning_rate, global_step, params.decay_steps, 0.5,", "\t\t\t\t\tstaircase=True)", "\t\t\t\treturn learning_rate", "", "\t\t\ttrain_op = tf.contrib.layers.optimize_loss(", "\t\t\t\tloss=loss,", "\t\t\t\tglobal_step=global_step,", "\t\t\t\tlearning_rate=params.learning_rate,", "\t\t\t\toptimizer=params.optimizer,", "\t\t\t\tlearning_rate_decay_fn=_decay)", "", "\t\treturn tf.contrib.learn.ModelFnOps(", "\t\t\tmode=mode,", "\t\t\tpredictions=predictions,", "\t\t\tloss=loss,", "\t\t\ttrain_op=train_op)", "", "\treturn _model_fn", "", "def experiment_fn(run_config, hparams):", "\t\"\"\"Constructs an experiment object.\"\"\"", "\testimator = tf.contrib.learn.Estimator(", "\t\tmodel_fn=make_model_fn(), config=run_config, params=hparams)", "\treturn tf.contrib.learn.Experiment(", "\t\testimator=estimator,", "\t\ttrain_input_fn=make_input_fn(tf.estimator.ModeKeys.TRAIN, hparams),", "\t\teval_input_fn=make_input_fn(tf.estimator.ModeKeys.EVAL, hparams),", "\t\teval_metrics=MODELS[FLAGS.model].eval_metrics(hparams),", "\t\teval_steps=FLAGS.eval_steps,", "\t\tmin_eval_frequency=FLAGS.eval_frequency)", "", "def main(unused_argv):", "\t\"\"\"Main entry point.\"\"\"", "\tif FLAGS.output_dir:", "\t\tmodel_dir = FLAGS.output_dir", "\telse:", "\t\tmodel_dir = \"output/%s_%s\" % (FLAGS.model, FLAGS.dataset)", "", "\tDATASETS[FLAGS.dataset].prepare()", "", "\tsession_config = tf.ConfigProto()", "\tsession_config.allow_soft_placement = True", "\tsession_config.gpu_options.allow_growth = True", "\trun_config = tf.contrib.learn.RunConfig(", "\t\tmodel_dir=model_dir,", "\t\tsave_summary_steps=FLAGS.save_summary_steps,", "\t\tsave_checkpoints_steps=FLAGS.save_checkpoints_steps,", "\t\tsave_checkpoints_secs=None,", "\t\tsession_config=session_config)", "", "\ttf.contrib.learn.learn_runner.run(", "\t\texperiment_fn=experiment_fn,", "\t\trun_config=run_config,", "\t\tschedule=FLAGS.schedule,", "\t\thparams=get_params())", "", "if __name__ == \"__main__\":", "\ttf.app.run()", ""]

[["tf:trainer".actions]]
action="Load"
args=["numpy","tensorflow"]


["tf:mnist"]
name = "MNIST dataset"
description = "MNIST dataset preprocessing and specifications"
type = "Template"
body = ["\"\"\"MNIST dataset preprocessing and specifications.\"\"\"", "", "from __future__ import absolute_import", "from __future__ import division", "from __future__ import print_function", "", "import gzip", "import numpy as np", "import os", "from six.moves import urllib", "import struct", "import tensorflow as tf", "", "REMOTE_URL = \"http://yann.lecun.com/exdb/mnist/\"", "LOCAL_DIR = \"data/mnist/\"", "TRAIN_IMAGE_URL = \"train-images-idx3-ubyte.gz\"", "TRAIN_LABEL_URL = \"train-labels-idx1-ubyte.gz\"", "TEST_IMAGE_URL = \"t10k-images-idx3-ubyte.gz\"", "TEST_LABEL_URL = \"t10k-labels-idx1-ubyte.gz\"", "", "IMAGE_SIZE = 28", "NUM_CLASSES = 10", "", "def get_params():", "\t\"\"\"Dataset params.\"\"\"", "\treturn {", "\t\t\"num_classes\": NUM_CLASSES,", "\t}", "", "def prepare():", "\t\"\"\"This function will be called once to prepare the dataset.\"\"\"", "\tif not os.path.exists(LOCAL_DIR):", "\t\tos.makedirs(LOCAL_DIR)", "\tfor name in [", "\t\t\tTRAIN_IMAGE_URL,", "\t\t\tTRAIN_LABEL_URL,", "\t\t\tTEST_IMAGE_URL,", "\t\t\tTEST_LABEL_URL]:", "\t\tif not os.path.exists(LOCAL_DIR + name):", "\t\t\turllib.request.urlretrieve(REMOTE_URL + name, LOCAL_DIR + name)", "", "def read(split):", "\t\"\"\"Create an instance of the dataset object.\"\"\"", "\timage_urls = {", "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_IMAGE_URL,", "\t\ttf.estimator.ModeKeys.EVAL: TEST_IMAGE_URL", "\t}[split]", "\tlabel_urls = {", "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_LABEL_URL,", "\t\ttf.estimator.ModeKeys.EVAL: TEST_LABEL_URL", "\t}[split]", "", "\twith gzip.open(LOCAL_DIR + image_urls, \"rb\") as f:", "\t\tmagic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))", "\t\timages = np.frombuffer(f.read(num * rows * cols), dtype=np.uint8)", "\t\timages = np.reshape(images, [num, rows, cols, 1])", "\t\tprint(\"Loaded %d images of size [%d, %d].\" % (num, rows, cols))", "", "\twith gzip.open(LOCAL_DIR + label_urls, \"rb\") as f:", "\t\tmagic, num = struct.unpack(\">II\", f.read(8))", "\t\tlabels = np.frombuffer(f.read(num), dtype=np.int8)", "\t\tprint(\"Loaded %d labels.\" % num)", "", "\treturn tf.contrib.data.Dataset.from_tensor_slices((images, labels))", "", "def parse(image, label):", "\t\"\"\"Parse input record to features and labels.\"\"\"", "\timage = tf.to_float(image) / 255.0", "\tlabel = tf.to_int64(label)", "\treturn {\"image\": image}, {\"label\": label}", ""]

[["tf:mnist".actions]]
action="Load"
args=["numpy","tensorflow"]


["tf:cifar10"]
name = "CIFAR10 dataset"
description = "CIFAR10 dataset preprocessing and specifications"
type = "Template"
body = ["\"\"\"Cifar10 dataset preprocessing and specifications.\"\"\"", "", "from __future__ import absolute_import", "from __future__ import division", "from __future__ import print_function", "", "import os", "import tarfile", "import numpy as np", "from six.moves import cPickle", "from six.moves import urllib", "import tensorflow as tf", "", "REMOTE_URL = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"", "LOCAL_DIR = os.path.join(\"data/cifar10/\")", "ARCHIVE_NAME = \"cifar-10-python.tar.gz\"", "DATA_DIR = \"cifar-10-batches-py/\"", "TRAIN_BATCHES = [\"data_batch_%d\" % (i + 1) for i in range(5)]", "TEST_BATCHES = [\"test_batch\"]", "", "IMAGE_SIZE = 32", "NUM_CLASSES = 10", "", "def get_params():", "\t\"\"\"Return dataset parameters.\"\"\"", "\treturn {", "\t\t\"image_size\": IMAGE_SIZE,", "\t\t\"num_classes\": NUM_CLASSES,", "\t}", "", "def prepare():", "\t\"\"\"Download the cifar dataset.\"\"\"", "\tif not os.path.exists(LOCAL_DIR):", "\t\tos.makedirs(LOCAL_DIR)", "\tif not os.path.exists(LOCAL_DIR + ARCHIVE_NAME):", "\t\tprint(\"Downloading...\")", "\t\turllib.request.urlretrieve(REMOTE_URL, LOCAL_DIR + ARCHIVE_NAME)", "\tif not os.path.exists(LOCAL_DIR + DATA_DIR):", "\t\tprint(\"Extracting files...\")", "\t\ttar = tarfile.open(LOCAL_DIR + ARCHIVE_NAME)", "\t\ttar.extractall(LOCAL_DIR)", "\t\ttar.close()", "", "def read(split):", "\t\"\"\"Create an instance of the dataset object.\"\"\"", "\t\"\"\"An iterator that reads and returns images and labels from cifar.\"\"\"", "\tbatches = {", "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_BATCHES,", "\t\ttf.estimator.ModeKeys.EVAL: TEST_BATCHES", "\t}[split]", "", "\tall_images = []", "\tall_labels = []", "", "\tfor batch in batches:", "\t\twith open(\"%s%s%s\" % (LOCAL_DIR, DATA_DIR, batch), \"rb\") as fo:", "\t\t\tdict = cPickle.load(fo)", "\t\t\timages = np.array(dict[\"data\"])", "\t\t\tlabels = np.array(dict[\"labels\"])", "", "\t\t\tnum = images.shape[0]", "\t\t\timages = np.reshape(images, [num, 3, IMAGE_SIZE, IMAGE_SIZE])", "\t\t\timages = np.transpose(images, [0, 2, 3, 1])", "\t\t\tprint(\"Loaded %d examples.\" % num)", "", "\t\t\tall_images.append(images)", "\t\t\tall_labels.append(labels)", "", "\tall_images = np.concatenate(all_images)", "\tall_labels = np.concatenate(all_labels)", "", "\treturn tf.contrib.data.Dataset.from_tensor_slices((all_images, all_labels))", "", "def parse(image, label):", "\t\"\"\"Parse input record to features and labels.\"\"\"", "\timage = tf.to_float(image) / 255.0", "\timage = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])", "\treturn {\"image\": image}, {\"label\": label}", ""]

[["tf:cifar10".actions]]
action="Load"
args=["numpy","tensorflow"]


["tf:cifar100"]
name = "CIFAR100 dataset"
description = "CIFAR100 dataset preprocessing and specifications"
type = "Template"
body = ["\"\"\"Cifar100 dataset preprocessing and specifications.\"\"\"", "", "from __future__ import absolute_import", "from __future__ import division", "from __future__ import print_function", "", "import os", "import tarfile", "import numpy as np", "from six.moves import cPickle", "from six.moves import urllib", "import tensorflow as tf", "", "REMOTE_URL = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"", "LOCAL_DIR = os.path.join(\"data/cifar100/\")", "ARCHIVE_NAME = \"cifar-100-python.tar.gz\"", "DATA_DIR = \"cifar-100-python/\"", "TRAIN_BATCHES = [\"train\"]", "TEST_BATCHES = [\"test\"]", "", "IMAGE_SIZE = 32", "NUM_CLASSES = 100", "", "def get_params():", "\t\"\"\"Return dataset parameters.\"\"\"", "\treturn {", "\t\t\"image_size\": IMAGE_SIZE,", "\t\t\"num_classes\": NUM_CLASSES,", "\t}", "", "def prepare():", "\t\"\"\"Download the cifar 100 dataset.\"\"\"", "\tif not os.path.exists(LOCAL_DIR):", "\t\tos.makedirs(LOCAL_DIR)", "\tif not os.path.exists(LOCAL_DIR + ARCHIVE_NAME):", "\t\tprint(\"Downloading...\")", "\t\turllib.request.urlretrieve(REMOTE_URL, LOCAL_DIR + ARCHIVE_NAME)", "\tif not os.path.exists(LOCAL_DIR + DATA_DIR):", "\t\tprint(\"Extracting files...\")", "\t\ttar = tarfile.open(LOCAL_DIR + ARCHIVE_NAME)", "\t\ttar.extractall(LOCAL_DIR)", "\t\ttar.close()", "", "def read(split):", "\t\"\"\"Create an instance of the dataset object.\"\"\"", "\tbatches = {", "\t\ttf.estimator.ModeKeys.TRAIN: TRAIN_BATCHES,", "\t\ttf.estimator.ModeKeys.EVAL: TEST_BATCHES", "\t}[split]", "", "\tall_images = []", "\tall_labels = []", "", "\tfor batch in batches:", "\t\twith open(\"%s%s%s\" % (LOCAL_DIR, DATA_DIR, batch), \"rb\") as fo:", "\t\t\tdict = cPickle.load(fo)", "\t\t\timages = np.array(dict[\"data\"])", "\t\t\tlabels = np.array(dict[\"fine_labels\"])", "", "\t\t\tnum = images.shape[0]", "\t\t\timages = np.reshape(images, [num, 3, IMAGE_SIZE, IMAGE_SIZE])", "\t\t\timages = np.transpose(images, [0, 2, 3, 1])", "\t\t\tprint(\"Loaded %d examples.\" % num)", "", "\t\t\tall_images.append(images)", "\t\t\tall_labels.append(labels)", "", "\tall_images = np.concatenate(all_images)", "\tall_labels = np.concatenate(all_labels)", "", "\treturn tf.contrib.data.Dataset.from_tensor_slices((all_images, all_labels))", "", "def parse(image, label):", "\t\"\"\"Parse input record to features and labels.\"\"\"", "\timage = tf.to_float(image) / 255.0", "\timage = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3])", "\treturn {\"image\": image}, {\"label\": label}", ""]

[["tf:cifar100".actions]]
action="Load"
args=["numpy","tensorflow"]
